{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NithinUppara2003/Projects/blob/main/UAV_Dataset_Reading_%26%26_Augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'uavid-v1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1077128%2F1812990%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240219%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240219T192412Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2b47a1e8db6a6a5c58911e678ab238f909d239479bfa6f53f853f2b03dbdb174fc2ac116b1136fe6a12042d7ee8a71f7c7440cf2014acf8ae017b900b9a6fbf911cebaff86e81a967fa8dad687e5ffd92505da4677f2af0bc45136fbdc38544ffacab4795fc77b38bcac9dd9f5a18d63fac92377aaf207b2988db1cd00b13758a6b442f7e74fa719e2b03ad9bf3de10b59754a16962f82e7b553fa5b33da3819407c867ea85cf2c7ae9e31d596cc255f86815f99c95391baaceff004fb37f24d01860433e2f6574d23db01878b39b9a8859e2bcba996a280907a01cc1c6bf50b68f62c50208460011cada1d46aff02d65c1fd47375ae607752d5361239f9efbd'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "lI2XmvQJ-tFO"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Hav34gZM-tFQ"
      },
      "cell_type": "markdown",
      "source": [
        "* [Data Resizing](#Resizing)\n",
        "* [Label Flatten](#Label_Flattening)\n",
        "* [Data Augmentation](#Augmentation)"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "NerBPBBc-tFR"
      },
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "\n",
        "# Test\n",
        "import glob\n",
        "from matplotlib.pyplot import imshow\n",
        "\n",
        "import os\n",
        "import io\n",
        "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#    for filename in filenames:\n",
        "#        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "l8ZJTBr8-tFR"
      },
      "cell_type": "code",
      "source": [
        "## Global Parameters\n",
        "tfrecord_name = \"uav_images\"\n",
        "train_img_path = \"/kaggle/input/uavid-v1/uavid_train/seq1/Images/\"\n",
        "train_label_path= \"/kaggle/input/uavid-v1/uavid_train/seq1/Labels/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-8MYantF-tFR"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"train_images = glob.glob(train_img_path + '*.png')\n",
        "train_labels = glob.glob(train_label_path + '*.png')\n",
        "# print(images)\n",
        "for image in train_images[:1]:\n",
        "    img = Image.open(image)\n",
        "    imshow(np.asarray(img))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Sdn0Q-DA-tFR"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"img = Image.open(train_images[5])\n",
        "imshow(np.asarray(img))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Uy9TnNQY-tFR"
      },
      "cell_type": "code",
      "source": [
        "\"\"\"img_labelled = Image.open(train_labels[5])\n",
        "imshow(np.asarray(img_labelled))\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k1mY8o9Y-tFR"
      },
      "cell_type": "markdown",
      "source": [
        "# Resizing"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "v9XAHyP1-tFS"
      },
      "cell_type": "code",
      "source": [
        "def resize_all_train_images_path(train_img_path):\n",
        "    \"\"\"\n",
        "    Direkt olarak path'den alıp boyutlarını değiştiriyoruz.\n",
        "    \"\"\"\n",
        "    train_images = glob.glob(train_img_path + '*.png')\n",
        "    for index in range(len(train_images)):\n",
        "        train_images[index] = tf.keras.preprocessing.image.array_to_img(tf.image.resize(np.asarray(Image.open(train_images[index])), [256,256]))\n",
        "    return train_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FxRjvd8B-tFS"
      },
      "cell_type": "code",
      "source": [
        "def resize_all_train_images_list(train_images):\n",
        "    \"\"\"\n",
        "    Direkt olarak listeden alıp boyutlarını değiştiriyoruz.\n",
        "        Bu fonksiyon tfrecord data okunduktan sonra çağırabilinir.\n",
        "    \"\"\"\n",
        "\n",
        "    for index in range(len(train_images)):\n",
        "        train_images[index] = tf.keras.preprocessing.image.array_to_img(tf.image.resize(np.asarray(train_images[index], [256,256])))\n",
        "    # test edilecek -> return train_images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "V4pvTd4Q-tFS"
      },
      "cell_type": "code",
      "source": [
        "train_images = resize_all_train_images_path(train_img_path=train_img_path)\n",
        "label_images = resize_all_train_images_path(train_img_path=train_label_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nf9501Ji-tFS"
      },
      "cell_type": "code",
      "source": [
        "# Test 200 file is 256x256\n",
        "total = 0\n",
        "for index in range(len(train_images)):\n",
        "    if train_images[index].height == 256 and train_images[0].width == 256:\n",
        "        total += 1\n",
        "\n",
        "if total==200:\n",
        "    print(\"Tüm fotoğraflar resized edildi..\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8U8n0OTp-tFS"
      },
      "cell_type": "code",
      "source": [
        "np.asarray(train_images[1]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "88zAElkV-tFS"
      },
      "cell_type": "markdown",
      "source": [
        "## Augmentation\n",
        "[Reference](https://www.tensorflow.org/api_docs/python/tf/image#:~:text=TensorFlow%20provides%20functions%20to%20adjust,training%20set%20and%20reduce%20overfitting.)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "bd7PM_dx-tFS"
      },
      "cell_type": "code",
      "source": [
        "train_images[0] = tf.keras.preprocessing.image.array_to_img(tf.image.transpose(np.asarray(train_images[0])))\n",
        "label_images[0] = tf.keras.preprocessing.image.array_to_img(tf.image.transpose(np.asarray(label_images[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AWfwXGIP-tFS"
      },
      "cell_type": "code",
      "source": [
        "label_images[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "CyGwvgnR-tFS"
      },
      "cell_type": "code",
      "source": [
        "train_images[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FpFCtHAc-tFS"
      },
      "cell_type": "code",
      "source": [
        "train_images[0] = tf.keras.preprocessing.image.array_to_img(tf.image.random_brightness(np.asarray(train_images[0]), max_delta=0.2))\n",
        "label_images[0] = tf.keras.preprocessing.image.array_to_img(tf.image.random_brightness(np.asarray(label_images[0]), max_delta=0.2)) # Label a gerek var mı?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "gerJi-Be-tFT"
      },
      "cell_type": "code",
      "source": [
        "train_images[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VUUHuYp3-tFT"
      },
      "cell_type": "code",
      "source": [
        "label_images[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9A4pzFTZ-tFT"
      },
      "cell_type": "code",
      "source": [
        "train_images[0] = tf.keras.preprocessing.image.array_to_img(tf.image.resize_with_crop_or_pad(np.asarray(train_images[0]), target_height=))\n",
        "label_images[0] = tf.keras.preprocessing.image.array_to_img(tf.image.resize_with_crop_or_pad(np.asarray(label_images[0]), target_height=))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2MXt4bcv-tFT"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ag3cm0zD-tFT"
      },
      "cell_type": "markdown",
      "source": [
        "# Label_Flattening"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "e_BPtsoZ-tFT"
      },
      "cell_type": "code",
      "source": [
        "def flat_all_train_labels_path(train_label_path):\n",
        "    \"\"\"\n",
        "    Direkt olarak path'den alıp flatten uyguluyoruz..\n",
        "    \"\"\"\n",
        "\n",
        "    train_labels = resize_all_train_images_path(train_label_path)\n",
        "\n",
        "    for index in range(len(train_labels)):\n",
        "        train_labels[index] = tf.keras.preprocessing.image.array_to_img(tf.image.resize(np.asarray(train_labels[index]), [ 1, 196608]))\n",
        "    return train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-c1_5YNo-tFT"
      },
      "cell_type": "code",
      "source": [
        "def flat_all_train_labels_list(train_label_path):\n",
        "    \"\"\"\n",
        "    Direkt olarak list'den alıp flatten uyguluyoruz..\n",
        "    \"\"\"\n",
        "\n",
        "    train_labels = resize_all_train_images_list(train_label_path)\n",
        "\n",
        "    for index in range(len(train_labels)):\n",
        "        train_labels[index] = tf.keras.preprocessing.image.array_to_img(tf.image.resize(np.asarray(train_labels[index]), [ 1, 196608]))\n",
        "    return train_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "XFkf6Fez-tFT"
      },
      "cell_type": "code",
      "source": [
        "train_labels = flat_all_train_labels_path(train_label_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Fx0D9F-4-tFT"
      },
      "cell_type": "code",
      "source": [
        "print(train_labels[5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cDW8FrGu-tFT"
      },
      "cell_type": "markdown",
      "source": [
        "# Tüm dataları tfRecord yapıyoruz"
      ]
    },
    {
      "metadata": {
        "id": "-W_GLRR2-tFT"
      },
      "cell_type": "markdown",
      "source": [
        "*write_tfrecords(train_images, label_images, name_of_tfrecords)*"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "HUZmjkUZ-tFT"
      },
      "cell_type": "code",
      "source": [
        "#buf = io.BytesIO()\n",
        "#train_images[5].save(buf, format='JPEG')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7V8lYqdK-tFT"
      },
      "cell_type": "code",
      "source": [
        "#byte_im = buf.getvalue()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1kaS1ols-tFT"
      },
      "cell_type": "code",
      "source": [
        "#print(byte_im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "JECkrYEY-tFT"
      },
      "cell_type": "code",
      "source": [
        "def _bytes_feature(value):\n",
        "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "\n",
        "def _float_feature(value):\n",
        "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
        "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
        "\n",
        "\n",
        "def _int64_feature(value):\n",
        "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
        "\n",
        "\n",
        "def write_record(train_images, train_labels, tfrecord_name):\n",
        "    features = []\n",
        "    # Read image raw data, which will be embedded in the record file later.\n",
        "    for index in range(len(train_images)):\n",
        "        buf_img = io.BytesIO()\n",
        "        buf_label = io.BytesIO()\n",
        "\n",
        "        train_images[index].save(buf_img, format='PNG')\n",
        "\n",
        "\n",
        "\n",
        "        # temp = np.array(train_images[index])\n",
        "\n",
        "        image_string = buf_img.getvalue()\n",
        "\n",
        "\n",
        "        train_labels[index].save(buf_label, format='PNG')\n",
        "\n",
        "        # test\n",
        "\n",
        "        # temp = np.array(train_labels[index])\n",
        "\n",
        "        label = buf_label.getvalue()\n",
        "\n",
        "        # For each sample there are two features: image raw data, and label. Wrap them in a single dict.\n",
        "        feature = {\n",
        "            'label': _bytes_feature(label),\n",
        "            'image_raw': _bytes_feature(image_string),\n",
        "        }\n",
        "\n",
        "        features.append(feature)\n",
        "\n",
        "    # Create a `example` from the feature dict.\n",
        "    tf_examples = []\n",
        "    for i in range(len(train_images)):\n",
        "        tf_example = tf.train.Example(features=tf.train.Features(feature=features[i]))\n",
        "        tf_examples.append(tf_example)\n",
        "    # tf_example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
        "\n",
        "    # Write the serialized example to a record file.\n",
        "    with tf.io.TFRecordWriter(tfrecord_name + \".tfrecord\") as writer:\n",
        "        for index in range(len(train_images)):\n",
        "            writer.write(tf_examples[index].SerializeToString())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4rbX74-B-tFU"
      },
      "cell_type": "code",
      "source": [
        "write_record(train_images, label_images, tfrecord_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4gY8lvGD-tFU"
      },
      "cell_type": "code",
      "source": [
        "def read_record(tfrecord_name):\n",
        "    # Use dataset API to import date directly from TFRecord file.\n",
        "    raw_image_dataset = tf.data.TFRecordDataset(tfrecord_name + \".tfrecord\")\n",
        "\n",
        "    # Create a dictionary describing the features. The key of the dict should be the same with the key in writing function.\n",
        "    image_feature_description = {\n",
        "        'label': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "\n",
        "    # Define the parse function to extract a single example as a dict.\n",
        "    def _parse_image_function(example_proto):\n",
        "        # Parse the input tf.Example proto using the dictionary above.\n",
        "        features = tf.io.parse_single_example(example_proto, image_feature_description)\n",
        "        image = tf.io.decode_image(features['image_raw'], dtype=tf.uint8)\n",
        "        print(\"TYPE: {}\\tShape: {}\".format(type(image), image.shape))\n",
        "        #image.set_shape([3 * 256 * 256])\n",
        "        image = tf.reshape(image, [256, 256, 3])\n",
        "\n",
        "        label = tf.io.decode_image(features['label'], dtype=tf.uint8)\n",
        "        print(\"TYPE: {}\\tShape: {}\".format(type(label), label.shape))\n",
        "        #label.set_shape([ 1, 196608])\n",
        "        label = tf.reshape(label,[ 1, 196608, 1]) # Burası değiştirilecek.\n",
        "\n",
        "        return image,label\n",
        "\n",
        "    dataset = raw_image_dataset.map(_parse_image_function)\n",
        "    dataset = dataset.cache()\n",
        "    dataset = dataset.batch(8)\n",
        "\n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2Ll7nVly-tFU"
      },
      "cell_type": "code",
      "source": [
        "%xmode Plain"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "KCkqyfi0-tFU"
      },
      "cell_type": "code",
      "source": [
        "dataset = read_record(tfrecord_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kfP672jq-tFU"
      },
      "cell_type": "code",
      "source": [
        "type(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "yykCSN1q-tFU"
      },
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "TxIOyofT-tFX"
      },
      "cell_type": "code",
      "source": [
        "for x,y in dataset.take(1):\n",
        "    print(type(x), type(y))\n",
        "    x = x.numpy()\n",
        "    y = y.numpy()\n",
        "    print(type(x), type(y))\n",
        "    print(len(x))\n",
        "    print(x[0].shape)\n",
        "    print(y[0].shape)\n",
        "    img_neww = tf.keras.preprocessing.image.array_to_img(x[3])\n",
        "    label_neww = tf.keras.preprocessing.image.array_to_img(y[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "nkZwq67s-tFY"
      },
      "cell_type": "code",
      "source": [
        "img_neww"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "idZxY6j9-tFY"
      },
      "cell_type": "code",
      "source": [
        "label_neww"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "2_0ZyPRw-tFY"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "UAV Dataset Reading && Augmentation",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}